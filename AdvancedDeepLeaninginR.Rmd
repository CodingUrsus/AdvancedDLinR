---
title: "AdvancedDeepLearninginR"
author: "Austin Hammer"
date: "2022-12-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libs_and_data}
library(tidyverse)
library(keras)
library(here)
library(mlbench)
library(psych)
library(neuralnet)


# chapter3 data
data(BostonHousing)
boston_house <- BostonHousing %>%
  mutate_if(is.factor,as.numeric)
```

```{r chapter3_data}
# just testing it out with some regression-type tasks
n <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+b+lstat,
               data=boston_house,
               hidden=c(10,5),
               linear.output=F,
               lifesign = 'full',
               rep=1)

plot(n, col.hidden="darkgreen",
     col.hidden.synapse='darkgreen',
     show.weights=F,
     information=F,
     fill="lightblue")

# format data
boston_housing_data <- as.matrix(boston_house)
set.seed(1975)
# just splitting the data 70/30
indices <- sample(2, nrow(boston_housing_data), replace = T, prob=c(.7, .3))
training <- boston_housing_data[indices==1, 1:13]
test <- boston_housing_data[indices==2, 1:13]
trainingtarget <- boston_housing_data[indices==1, 14]
testtarget <- boston_housing_data[indices==2, 14]
```

```{r chapter3_model_train_and_test}
training_means <- colMeans(training)
train_sd <- apply(training, 2, sd)
training_data <- scale(training, center = training_means, scale=train_sd)
testing_data <- scale(test, center=training_means, scale=train_sd)
# note that some large datasets will need data imputation and NA handling

# the overall model looks like
housing_model <- keras_model_sequential()
housing_model %>%
 layer_dense(units = 10, activation = 'relu', input_shape = c(13)) %>%
 layer_dense(units = 5, activation = 'relu') %>%
 layer_dense(units = 1)

# compile it
housing_model %>%
  compile(loss='mse',
          optimizer = 'rmsprop',
          metrics = 'mse')

# train the model
boston_housing_fit_nn <- housing_model %>%
  fit(training,
      trainingtarget,
      epochs=100,
      batch_size=32,
      validation_split=0.2)

# evaluate and test it
housing_model %>%
  evaluate(test, testtarget)

housing_predictions <- housing_model %>%
  predict(test)

prediction_df <- data.frame("housing_predictions" = housing_predictions[1:150], "target" = testtarget[1:150]) 

ggplot(prediction_df, aes(x=housing_predictions, y=target)) +
  geom_point() +
  geom_smooth(method="lm", se=F) +
  theme_bw()

corr.test(prediction_df$housing_predictions, prediction_df$target)

# test it with a deeper architecture
model <- keras_model_sequential()
model %>%
 layer_dense(units = 100, activation = 'relu', input_shape = c(13)) %>%
 layer_dropout(rate = 0.4) %>%
 layer_dense(units = 50, activation = 'relu') %>%
 layer_dropout(rate = 0.3) %>%
 layer_dense(units = 20, activation = 'relu') %>%
 layer_dropout(rate = 0.2) %>%
 layer_dense(units = 1)

# Compile model
model %>% compile(loss = 'mse',
 optimizer = 'rmsprop',
 metrics = 'mae')
# Fit model
model_two <- model %>%
 fit(training,
 trainingtarget,
 epochs = 100,
 batch_size = 32,
 validation_split = 0.2)
plot(model_two)

## evaluate and test with the much more complex model
model %>% evaluate(test, testtarget)
pred <- model %>% predict(test)
pred_df <- data.frame("housing_predictions" = pred[1:150], "target" = testtarget[1:150]) 

ggplot(pred_df, aes(x=housing_predictions, y=target)) +
  geom_point() +
  geom_smooth(method="lm", se=F) +
  theme_bw()

corr.test(pred_df$housing_predictions, prediction_df$target)
# in general, consider modifying architecture or transforming input variable to see if there is improvement in performance
```

## Including Plots

You can also embed plots, for example:

```{r some_image_classification}
library(EBImage)
# a couple of notes for resizing the microbiome "images"
# 1. we won't really have to deal with image color channels, but usually need to
# 2. EBImage has a simple function to resize images, resize()
# 3. use to_categorical to one-hot encode

# get the fashion mnist data, this dataset has 60k images (28x28), and 10k for testing, and the data is presplit
fashion_mnist <- dataset_fashion_mnist()

trainx <- fashion_mnist$train$x
trainy <- fashion_mnist$train$y
testx <- fashion_mnist$test$x
testy <- fashion_mnist$test$y

# will now reshape and resize the data
trainy <- to_categorical(trainy, 10)
testy <- to_categorical(testy, 10)


# Model architecture
model <- keras_model_sequential()
model %>%
 layer_conv_2d(filters = 32,
 kernel_size = c(3,3),
 activation = 'relu',
 input_shape = c(28,28,1)) %>%
 layer_conv_2d(filters = 64,
 kernel_size = c(3,3),
 activation = 'relu') %>%
 layer_max_pooling_2d(pool_size = c(2,2)) %>%
 layer_dropout(rate = 0.25) %>%
 layer_flatten() %>%
 layer_dense(units = 64, activation = 'relu') %>%
 layer_dropout(rate = 0.25) %>%
 layer_dense(units = 10, activation = 'softmax')

# note that if you're working with a 3d image you'll need to use a 3d convolutional layer
# Compile model
model %>% compile(loss = 'categorical_crossentropy',
 optimizer = optimizer_adadelta(),
 metrics = 'accuracy')

# Fit model
model_one <- model %>% fit(trainx,
 trainy,
 epochs = 15,
 batch_size = 128,
 validation_split = 0.2)
plot(model_one)




```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
